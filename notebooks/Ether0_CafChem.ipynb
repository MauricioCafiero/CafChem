{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQdmgfNF-g1c"
   },
   "source": [
    "#CafChem tools for using Futurehouse's ether0 chatbot, a version of Mistral-Small-24B-Instruct-2501 finetuned for chemistry/medicinal chemistry applications.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MauricioCafiero/CafChem/blob/main/notebooks/Ether0_CafChem.ipynb)\n",
    "\n",
    "## This notebook allows you to:\n",
    "- Load either the full or a quantized version of ether0\n",
    "- perform inference with the model.\n",
    "\n",
    "## Requirements:\n",
    "- This notebook will install gguf\n",
    "- It will install all needed libraries.\n",
    "- Recommend the v2-8 TPU for memory (25 GB for quantzed, ~100 GB for full) but in either case will use CPU for inference. Inference will take a *long time!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyyAHEY09Ypf"
   },
   "source": [
    "## Use cases (from the [futurehouse Huggingface page](https://huggingface.co/futurehouse/ether0))\n",
    "\n",
    "- IUPAC name to SMILES\n",
    "- Molecular formula (Hill notation) to SMILES, optionally with constraints on functional groups\n",
    "- Modifying solubilities on given molecules (SMILES) by specific LogS, optionally with constraints about scaffolds/groups/similarity\n",
    "- Matching pKa to molecules, proposing molecules with a pKa, or modifying molecules to adjust pKa\n",
    "- Matching scent/smell to molecules and modifying molecules to adjust scent\n",
    "- Matching human cell receptor binding + mode (e.g., agonist) to molecule or modifying a molecule's binding effect. Trained from EveBio\n",
    "- ADME properties (e.g., MDDK efflux ratio, LD50)\n",
    "- GHS classifications (as words, not codes, like \"carcinogen\"). For example, \"modify this molecule to remove acute toxicity.\"\n",
    "- Quantitative LD50 in mg/kg\n",
    "- Proposing 1-step retrosynthesis from likely commercially available reagents\n",
    "- Predicting a reaction outcome\n",
    "- General natural language description of a specific molecule to that molecule (inverse molecule captioning)\n",
    "- Natural product elucidation (formula + organism to SMILES) - e.g, \"A molecule with formula C6H12O6 was isolated from Homo sapiens, what could it be?\"\n",
    "- Matching blood-brain barrier permeability (as a class) or modifying\n",
    "\n",
    "For example, you can ask \"Propose a molecule with a pKa of 9.2\" or \"Modify CCCCC(O)=OH to increase its pKa by about 1 unit.\" You cannot ask it \"What is the pKa of CCCCC(O)=OH?\" If you ask it questions that lie significantly beyond those tasks, it can fail. You can combine properties, although we haven't significantly benchmarked this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEQXnr_8_DGz"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wxp74of-6Q7"
   },
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DVM1LU3y7Npu"
   },
   "outputs": [],
   "source": [
    "!pip install gguf>=0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WM7jnqor_QTC",
    "outputId": "9a3d72e1-e41d-4343-d7d3-99850024fc8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gguf\n",
      "Version: 0.17.1\n",
      "Summary: Read and write ML models in GGUF for GGML\n",
      "Home-page: https://ggml.ai\n",
      "Author: GGML\n",
      "Author-email: ggml@ggml.ai\n",
      "License: \n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: numpy, pyyaml, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADFwMxun7C4Q"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grqUWuW_69RE",
    "outputId": "0fca8a47-ed43-4d7f-90b6-87b3230ab8fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch_xla/experimental/gru.py:113: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` or\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwkxRZrZ7E7q"
   },
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XbSDdL2_vLry"
   },
   "outputs": [],
   "source": [
    "def make_pipe(ether0_type: str):\n",
    "  '''\n",
    "    setup a pipe with either the full ether0 model or the quantized version\n",
    "\n",
    "    Args:\n",
    "      ether0_type: either full model or quantized version\n",
    "\n",
    "    Returns:\n",
    "      pipe: pipeline object with the chosen model\n",
    "  '''\n",
    "  if ether0_type == \"full\":\n",
    "    model_id = \"futurehouse/ether0\"\n",
    "    pipe = pipeline(\"text-generation\", model=model_id)\n",
    "  # elif ether0_type == \"other\":\n",
    "  #   model_id = \"redponike/ether0-GGUF\"\n",
    "  #   filename = \"ether0-Q6_K.gguf\"\n",
    "\n",
    "  #   torch_dtype = torch.float32 # could be torch.float16 or torch.bfloat16 too\n",
    "  #   tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\n",
    "  #   model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename, torch_dtype=torch_dtype)\n",
    "  elif ether0_type == \"quantized\":\n",
    "    model_id = \"DevQuasar/futurehouse.ether0-GGUF\"\n",
    "    filename = \"futurehouse.ether0.Q8_0.gguf\"\n",
    "\n",
    "    torch_dtype = torch.float32 # could also be torch.float16 or torch.bfloat16\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename, torch_dtype=torch_dtype)\n",
    "\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Model pipeline has been initialized!\")\n",
    "\n",
    "  return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_7v9vPl_K3W"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "37a97538e76549c88531b4097ca8d378",
      "28635dc4c6c0428a846eed823a90461f",
      "4ecf4ad54ecf4ac38c178822000197fe",
      "3930785a45ac49cfb824b7c77587641f",
      "71fad1b9dab040e293e21dc93a691b9e",
      "17e54585d62c4aa0aa83deee80e396ab",
      "394834393d714a56a06711437ec5f96c",
      "7fc0f863b6414ca7a367c5b0cbaf63e1",
      "c043ed7ba1cf4a4fb33b0b8bf0b848b3",
      "18b4f815f1bb4ba3b81e63a56fa0ef78",
      "8ca3cccd20ca4eb2a12f1192f210ebd4",
      "43080f51ed974553bbe6fec82edb58de",
      "1237563f19c64515a2af1abedd56484f",
      "52060ec418ee41e980d0497440f344be",
      "a1e49b753cd44978a3ad4c0f05dede03",
      "6d576072f8724fcfb5a78dbea5057708",
      "ec5c5d30879944c8aa18424d49003555",
      "79474bc936894e2c9b6b3cc246e8b30c",
      "0be589b42d5f4c659ded9a684d359cdb",
      "2d35dd85184e421db1a3e938851589ad",
      "48c3eab28f214aa9a6a992a19d4a3a54",
      "9e5adb28678a4f4ea20b35c52e8da301"
     ]
    },
    "id": "DMhLRTPW6vIY",
    "outputId": "0cb6cd50-3080-4d20-837a-55095c55ebc3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a97538e76549c88531b4097ca8d378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "futurehouse.ether0.Q8_0.gguf:   0%|          | 0.00/25.1G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43080f51ed974553bbe6fec82edb58de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting and de-quantizing GGUF tensors...:   0%|          | 0/363 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pipeline has been initialized!\n"
     ]
    }
   ],
   "source": [
    "quant_pipe = make_pipe(\"quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJkk4DSQ85e6",
    "outputId": "f01f9924-e84b-4864-d4ae-26ff1116b7da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propose a molecule with a pKa of 10.19, an H-bond donor, and acceptor, and a benzyl group.\n",
      "\n",
      "Let's consider benzyl alcohol, which has a pKa around 10.06. Adding a methyl group to the alcohol would make it 2-methoxybenzyl alcohol (pKa ~10.21). However, the user's pKa is 10.19, which is slightly lower. Let's consider a structure with an amide or ester group.\n",
      "\n",
      "Consider a ketone like benzyl acetone, which has a pKa around 18.93. Adding hydroxyl and amine groups would lower the pKa, but it might not reach 10.19. Consider a substituted benzyl alcohol.\n",
      "\n",
      "Consider benzylamine derivatives. Benzylamine is C7H9N. Adding a hydroxyl group and another substituent. Consider a pKa around 10.19.\n",
      "\n",
      "Consider a substituted benzene ring with an amide or ester. For instance, a benzene ring with a methyl ester (COOCH3) and a pKa around 10.19. Adding a hydroxyl group.\n",
      "\n",
      "Consider a substituted benzene ring with a hydroxyl, an amine, and another group. For instance, a benzene ring with -OH, -NH2, and\n"
     ]
    }
   ],
   "source": [
    "result = quant_pipe(\"Propose a molecule with a pKa of 10.1\")\n",
    "\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2KJrYltAfrA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "TEQXnr_8_DGz",
    "X_7v9vPl_K3W",
    "QH51WjuTdPgI",
    "Yxkpb_44Wm-E"
   ],
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
